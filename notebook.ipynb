{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Edge Reasoning Evaluation Framework\n",
        "This notebook provides an interactive workflow for reproducing the evaluation results in our paper. It supports both **server** and **Tegra** platforms.\n",
        "\n",
        "### Quick Navigation\n",
        "1. [Environment Setup](#setup)\n",
        "2. [Platform setup](#platform)\n",
        "3. [Server Evaluations](#server)\n",
        "4. [Tegra Evaluations](#tegra)\n",
        "5. [Results Processing](#results)\n",
        "6. [Analytical Models](#analytical)\n",
        "7. [Validation](#validation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup {#setup}\n",
        "\n",
        "Dtermine platform and set up the appropriate environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "* device information:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "928.72s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/modfi/models/edgereasoning/.venv/bin/python setup.py --info-only\n",
            "============================================================\n",
            "Environment Setup\n",
            "============================================================\n",
            "Auto-detected platform: server\n",
            "\n",
            "Device Information:\n",
            "  platform: server\n",
            "  python_version: 3.12.3\n",
            "  system: Linux\n",
            "  machine: x86_64\n",
            "  processor: x86_64\n",
            "  in_container: False\n",
            "  gpus:\n",
            "    GPU 0: NVIDIA RTX A6000 (49140 MiB)\n",
            "    GPU 1: NVIDIA RTX A6000 (49140 MiB)\n"
          ]
        }
      ],
      "source": [
        "# Platform detection using our make infrastructure\n",
        "platform = !make platform\n",
        "platform = platform[0].split(\":\")[-1].strip()\n",
        "print(\"\\n* device information:\")\n",
        "!make info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Server Evaluations\n",
        "\n",
        "For server platforms, we can run MMLU and Planner evaluations with different modes:\n",
        "\n",
        "### Available modes:\n",
        "- **base**: Full reasoning evaluation (4096 tokens)\n",
        "- **budget**: Budget evaluation (configurable tokens)\n",
        "- **noreasoning**: Direct answer selection\n",
        "- **scale**: Parameter scaling experiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  SERVER EVALUATION OPTIONS\n",
            "\n",
            " Run evaluations using make commands:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1677.06s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting MMLU Evaluation - Mode: budget\n",
            "Time: Fri Aug 22 11:43:05 PM PDT 2025\n",
            "Working directory: /home/modfi/models/edgereasoning/eval/server/mmlu\n",
            "\n",
            "* Running budget evaluation...\n",
            "INFO 08-22 23:43:08 [__init__.py:241] Automatically detected platform cuda.\n",
            "Starting Budget MMLU Evaluation - ALL SUBJECTS\n",
            "================================================\n",
            "Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
            "Config: configs/budget.yaml\n",
            "Output base: /home/modfi/models/edgereasoning/data/mmlu/server/DeepSeek-R1-Qwen-1.5B/mmlu_20250822_234309_budget_DeepSeek-R1-Distill-Qwen-1.5B\n",
            "Timestamp: 20250822_234309\n",
            "\n",
            "* Setting up model...\n",
            "Setting up model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
            "Loading tokenizer: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
            "Loading VLLM model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
            "INFO 08-22 23:43:11 [utils.py:326] non-default args: {'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 1024, 'gpu_memory_utilization': 0.6, 'show_hidden_metrics_for_version': '0.9.0', 'collect_detailed_traces': ['all']}\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "INFO 08-22 23:43:17 [__init__.py:711] Resolved architecture: Qwen2ForCausalLM\n",
            "INFO 08-22 23:43:17 [__init__.py:1750] Using max model len 1024\n",
            "WARNING 08-22 23:43:17 [arg_utils.py:1770] --otlp-traces-endpoint is not supported by the V1 Engine. Falling back to V0. \n",
            "INFO 08-22 23:43:17 [llm_engine.py:222] Initializing a V0 LLM engine (v0.10.1.1) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version='0.9.0', otlp_traces_endpoint=None, collect_detailed_traces=['all']), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{\"enable_fusion\":false,\"enable_noop\":false},\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
            "INFO 08-22 23:43:18 [cuda.py:436] Using Flash Attention backend.\n",
            "INFO 08-22 23:43:19 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "INFO 08-22 23:43:19 [model_runner.py:1080] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
            "INFO 08-22 23:43:19 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
            "INFO 08-22 23:43:20 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.37s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.37s/it]\n",
            "\n",
            "INFO 08-22 23:43:29 [default_loader.py:262] Loading weights took 9.46 seconds\n",
            "INFO 08-22 23:43:29 [model_runner.py:1112] Model loading took 3.3461 GiB and 9.937351 seconds\n",
            "INFO 08-22 23:43:30 [worker.py:295] Memory profiling takes 0.48 seconds\n",
            "INFO 08-22 23:43:30 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.40GiB) x gpu_memory_utilization (0.60) = 28.44GiB\n",
            "INFO 08-22 23:43:30 [worker.py:295] model weights take 3.35GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 23.65GiB.\n",
            "INFO 08-22 23:43:30 [executor_base.py:114] # cuda blocks: 55344, # CPU blocks: 9362\n",
            "INFO 08-22 23:43:30 [executor_base.py:119] Maximum concurrency for 1024 tokens per request: 864.75x\n",
            "INFO 08-22 23:43:35 [model_runner.py:1383] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:12<00:00,  2.87it/s]\n",
            "INFO 08-22 23:43:48 [model_runner.py:1535] Graph capturing finished in 12 secs, took 0.22 GiB\n",
            "INFO 08-22 23:43:48 [llm_engine.py:417] init engine (profile, create kv cache, warmup model) took 18.13 seconds\n",
            "INFO 08-22 23:43:48 [llm.py:298] Supported_tasks: ['generate']\n",
            "Model loaded successfully\n",
            "Model ready for evaluation\n",
            "INFO:src.data_loaders.mmlu_loader:Found 30 MMLU subjects\n",
            "* Found 30 subjects to evaluate\n",
            "Subjects: anatomy, business_ethics, clinical_knowledge, college_chemistry, college_computer_science...\n",
            "\n",
            "\n",
            "============================================================\n",
            "[1/30] Evaluating subject: anatomy\n",
            "============================================================\n",
            "Loading subject: anatomy\n",
            "INFO:src.data_loaders.mmlu_loader:Loading dataset: anatomy (test)\n",
            "INFO:src.data_loaders.mmlu_loader:Loaded 100 questions for anatomy\n",
            "Evaluating 100 questions (budget mode)\n",
            "Initialized NVML monitoring for 2 GPUs\n",
            "  GPU 0: NVIDIA RTX A6000\n",
            "  GPU 1: NVIDIA RTX A6000\n",
            "Started NVML GPU telemetry: budget_anatomy_20250822_234351\n",
            "Processing question 1/100 (budget)\n",
            "Adding requests: 100%|███████████████████████████| 1/1 [00:00<00:00, 454.22it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  4.44it/s, est. speed input: 519.66\n",
            "Processing question 2/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1273.32it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 226.37\n",
            "Processing question 3/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1408.90it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.74it/s, est. speed input: 418.89\n",
            "Processing question 4/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1470.14it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.69it/s, est. speed input: 358.08\n",
            "Processing question 5/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1703.62it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.82it/s, est. speed input: 220.61\n",
            "Processing question 6/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1736.77it/s]\n",
            "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, oINFO 08-22 23:43:53 [metrics.py:386] Avg prompt throughput: 167.3 tokens/s, Avg generation throughput: 72.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.50it/s, est. speed input: 181.26\n",
            "Processing question 7/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1854.25it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.69it/s, est. speed input: 293.22\n",
            "Processing question 8/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1568.55it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.22it/s, est. speed input: 292.72\n",
            "Processing question 9/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1914.33it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  4.77it/s, est. speed input: 525.31\n",
            "Processing question 10/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1938.22it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  4.79it/s, est. speed input: 537.13\n",
            "Processing question 11/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1728.18it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 128.52\n",
            "Processing question 12/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1800.13it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 210.94\n",
            "Processing question 13/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1571.49it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.80it/s, est. speed input: 283.16\n",
            "Processing question 14/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1533.57it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.89it/s, est. speed input: 323.19\n",
            "Processing question 15/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1847.71it/s]\n",
            "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, oINFO 08-22 23:43:58 [metrics.py:386] Avg prompt throughput: 232.9 tokens/s, Avg generation throughput: 149.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 153.20\n",
            "Processing question 16/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1748.36it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 189.80\n",
            "Processing question 17/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1654.56it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.01it/s, est. speed input: 306.18\n",
            "Processing question 18/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 2014.56it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 134.40\n",
            "Processing question 19/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1842.84it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  3.27it/s, est. speed input: 379.45\n",
            "Processing question 20/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1975.65it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.29it/s, est. speed input: 249.78\n",
            "Processing question 21/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1605.78it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.54it/s, est. speed input: 256.72\n",
            "Processing question 22/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 2009.73it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  6.16it/s, est. speed input: 678.37\n",
            "Processing question 23/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1447.81it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.58it/s, est. speed input: 292.89\n",
            "Processing question 24/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1508.20it/s]\n",
            "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, oINFO 08-22 23:44:03 [metrics.py:386] Avg prompt throughput: 253.7 tokens/s, Avg generation throughput: 149.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 197.14\n",
            "Processing question 25/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 2010.69it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  6.69it/s, est. speed input: 750.28\n",
            "Processing question 26/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1927.53it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.62it/s, est. speed input: 231.06\n",
            "Processing question 27/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1500.65it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.69it/s, est. speed input: 371.19\n",
            "Processing question 28/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 2003.01it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.02it/s, est. speed input: 252.62\n",
            "Processing question 29/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1716.16it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 181.54\n",
            "Processing question 30/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1909.11it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.49it/s, est. speed input: 169.89\n",
            "Processing question 31/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1720.39it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.40it/s, est. speed input: 372.30\n",
            "Processing question 32/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1556.91it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.36it/s, est. speed input: 214.73\n",
            "Processing question 33/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 2028.19it/s]\n",
            "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, oINFO 08-22 23:44:08 [metrics.py:386] Avg prompt throughput: 237.9 tokens/s, Avg generation throughput: 149.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.22it/s, est. speed input: 242.20\n",
            "Processing question 34/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1666.39it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  4.02it/s, est. speed input: 656.58\n",
            "Processing question 35/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1872.46it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 273.65\n",
            "Processing question 36/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1526.87it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.02it/s, est. speed input: 353.48\n",
            "Processing question 37/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1677.05it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.01it/s, est. speed input: 271.24\n",
            "Processing question 38/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1113.73it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.69it/s, est. speed input: 423.18\n",
            "Processing question 39/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1736.77it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.80it/s, est. speed input: 243.76\n",
            "Processing question 40/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1610.10it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 165.29\n",
            "Processing question 41/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1774.24it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 173.32\n",
            "Processing question 42/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1871.62it/s]\n",
            "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, oINFO 08-22 23:44:13 [metrics.py:386] Avg prompt throughput: 258.9 tokens/s, Avg generation throughput: 148.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 142.85\n",
            "Processing question 43/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1766.02it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  3.00it/s, est. speed input: 447.40\n",
            "Processing question 44/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1931.08it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  3.48it/s, est. speed input: 379.48\n",
            "Processing question 45/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1822.03it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.71it/s, est. speed input: 211.52\n",
            "Processing question 46/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1671.04it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 162.70\n",
            "Processing question 47/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1602.71it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.07it/s, est. speed input: 308.56\n",
            "Processing question 48/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1894.45it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.84it/s, est. speed input: 369.52\n",
            "Processing question 49/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1907.37it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.47it/s, est. speed input: 288.93\n",
            "Processing question 50/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1844.46it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.17it/s, est. speed input: 134.64\n",
            "Processing question 51/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1855.89it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  5.48it/s, est. speed input: 592.28\n",
            "Processing question 52/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1847.71it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.55it/s, est. speed input: 298.61\n",
            "Processing question 53/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1779.51it/s]\n",
            "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, oINFO 08-22 23:44:18 [metrics.py:386] Avg prompt throughput: 278.7 tokens/s, Avg generation throughput: 148.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.55it/s, est. speed input: 215.06\n",
            "Processing question 54/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1854.25it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.51it/s, est. speed input: 301.39\n",
            "Processing question 55/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1592.98it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.70it/s, est. speed input: 287.03\n",
            "Processing question 56/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1870.79it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 160.92\n",
            "Processing question 57/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1431.01it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.89it/s, est. speed input: 439.07\n",
            "Processing question 58/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1897.88it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.84it/s, est. speed input: 221.46\n",
            "Processing question 59/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1898.73it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.04it/s, est. speed input: 236.40\n",
            "Processing question 60/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1831.57it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.32it/s, est. speed input: 269.24\n",
            "Processing question 61/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1809.45it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  5.08it/s, est. speed input: 666.64\n",
            "Processing question 62/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1842.84it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.04it/s, est. speed input: 228.32\n",
            "Processing question 63/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1501.72it/s]\n",
            "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, oINFO 08-22 23:44:23 [metrics.py:386] Avg prompt throughput: 255.7 tokens/s, Avg generation throughput: 148.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.09it/s, est. speed input: 247.22\n",
            "Processing question 64/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1913.46it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.31it/s, est. speed input: 281.85\n",
            "Processing question 65/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 2026.23it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  5.86it/s, est. speed input: 704.34\n",
            "Processing question 66/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1902.18it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  5.66it/s, est. speed input: 771.21\n",
            "Processing question 67/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1844.46it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  3.00it/s, est. speed input: 422.99\n",
            "Processing question 68/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1832.37it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 148.33\n",
            "Processing question 69/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1911.72it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.76it/s, est. speed input: 219.88\n",
            "Processing question 70/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1873.29it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.32it/s, est. speed input: 276.26\n",
            "Processing question 71/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1842.03it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.13it/s, est. speed input: 278.65\n",
            "Processing question 72/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1705.69it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.12it/s, est. speed input: 309.90\n",
            "Processing question 73/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 2037.06it/s]\n",
            "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, oINFO 08-22 23:44:28 [metrics.py:386] Avg prompt throughput: 255.7 tokens/s, Avg generation throughput: 148.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 133.01\n",
            "Processing question 74/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1846.08it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  5.09it/s, est. speed input: 545.40\n",
            "Processing question 75/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1863.31it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.94it/s, est. speed input: 365.16\n",
            "Processing question 76/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1093.12it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  3.18it/s, est. speed input: 629.37\n",
            "Processing question 77/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1972.86it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  3.06it/s, est. speed input: 363.84\n",
            "Processing question 78/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1964.55it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  3.18it/s, est. speed input: 372.20\n",
            "Processing question 79/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1800.90it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.85it/s, est. speed input: 387.32\n",
            "Processing question 80/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1705.69it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.77it/s, est. speed input: 346.23\n",
            "Processing question 81/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 2038.05it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  3.48it/s, est. speed input: 369.15\n",
            "Processing question 82/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1846.08it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.25it/s, est. speed input: 281.12\n",
            "Processing question 83/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1907.37it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  4.50it/s, est. speed input: 549.05\n",
            "Processing question 84/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1583.95it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 178.79\n",
            "Processing question 85/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1925.76it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.06it/s, est. speed input: 262.26\n",
            "Processing question 86/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1662.43it/s]\n",
            "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, oINFO 08-22 23:44:33 [metrics.py:386] Avg prompt throughput: 341.5 tokens/s, Avg generation throughput: 147.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 176.37\n",
            "Processing question 87/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1659.80it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 197.27\n",
            "Processing question 88/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1560.38it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.73it/s, est. speed input: 289.64\n",
            "Processing question 89/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1766.02it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.21it/s, est. speed input: 267.82\n",
            "Processing question 90/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1831.57it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.53it/s, est. speed input: 180.11\n",
            "Processing question 91/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1594.19it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.39it/s, est. speed input: 367.55\n",
            "Processing question 92/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1950.84it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  3.55it/s, est. speed input: 426.85\n",
            "Processing question 93/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1806.33it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.17it/s, est. speed input: 158.51\n",
            "Processing question 94/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1748.36it/s]\n",
            "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, oINFO 08-22 23:44:38 [metrics.py:386] Avg prompt throughput: 218.9 tokens/s, Avg generation throughput: 149.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.82it/s, est. speed input: 243.77\n",
            "Processing question 95/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1696.72it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.32it/s, est. speed input: 333.90\n",
            "Processing question 96/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1464.49it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  3.72it/s, est. speed input: 656.13\n",
            "Processing question 97/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1738.93it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  4.77it/s, est. speed input: 572.92\n",
            "Processing question 98/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1800.90it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.44it/s, est. speed input: 210.52\n",
            "Processing question 99/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1909.97it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  2.88it/s, est. speed input: 328.99\n",
            "Processing question 100/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1796.28it/s]\n",
            "Processed prompts: 100%|█| 1/1 [00:00<00:00,  1.42it/s, est. speed input: 212.39\n",
            "Performance metrics saved (AIME format): /home/modfi/models/edgereasoning/data/mmlu/server/DeepSeek-R1-Qwen-1.5B/mmlu_20250822_234309_budget_DeepSeek-R1-Distill-Qwen-1.5B/anatomy/performance_budget_anatomy_20250822_234351.csv\n",
            "\n",
            "=== Performance Summary ===\n",
            "Model: DeepSeek-R1-Distill-Qwen-1.5B\n",
            "Config: budget\n",
            "Evaluation: mmlu_anatomy_budget\n",
            "Total Questions: 100\n",
            "Total Time: 49530ms\n",
            "Avg Time/Question: 495.3ms\n",
            "Total Input Tokens: 13364\n",
            "Total Output Tokens: 7451\n",
            "Avg Tokens/Second: 150.4\n",
            "=========================\n",
            "\n",
            "=== GPU Telemetry Summary ===\n",
            "GPU 0:\n",
            "  Avg Temp: 62.5°C\n",
            "  Avg Power: 272.6W\n",
            "  Total Energy: 13486.0J\n",
            "  Avg Util: 86.9%\n",
            "  Avg Mem Util: 59.0%\n",
            "GPU 1:\n",
            "  Avg Temp: 33.0°C\n",
            "  Avg Power: 20.2W\n",
            "  Total Energy: 994.2J\n",
            "  Avg Util: 0.0%\n",
            "  Avg Mem Util: 1.3%\n",
            "GPU telemetry summary saved: /home/modfi/models/edgereasoning/data/mmlu/server/DeepSeek-R1-Qwen-1.5B/mmlu_20250822_234309_budget_DeepSeek-R1-Distill-Qwen-1.5B/anatomy/gpu_summary_budget_anatomy_20250822_234351.csv\n",
            "Stopped NVML GPU telemetry: budget_anatomy_20250822_234351\n",
            "Detailed results saved: /home/modfi/models/edgereasoning/data/mmlu/server/DeepSeek-R1-Qwen-1.5B/mmlu_20250822_234309_budget_DeepSeek-R1-Distill-Qwen-1.5B/anatomy/results_budget_anatomy_20250822_234351.json\n",
            "Detailed CSV saved: /home/modfi/models/edgereasoning/data/mmlu/server/DeepSeek-R1-Qwen-1.5B/mmlu_20250822_234309_budget_DeepSeek-R1-Distill-Qwen-1.5B/anatomy/detailed_results_budget_anatomy_20250822_234351.csv\n",
            "* anatomy completed!\n",
            "   Accuracy: 23.00%\n",
            "   Correct: 23/100\n",
            "   Avg Time/Question: 495.3ms\n",
            "\n",
            "============================================================\n",
            "[2/30] Evaluating subject: business_ethics\n",
            "============================================================\n",
            "Loading subject: business_ethics\n",
            "INFO:src.data_loaders.mmlu_loader:Loading dataset: business_ethics (test)\n",
            "INFO:src.data_loaders.mmlu_loader:Loaded 100 questions for business_ethics\n",
            "Evaluating 100 questions (budget mode)\n",
            "Initialized NVML monitoring for 2 GPUs\n",
            "  GPU 0: NVIDIA RTX A6000\n",
            "  GPU 1: NVIDIA RTX A6000\n",
            "Started NVML GPU telemetry: budget_business_ethics_20250822_234442\n",
            "Processing question 1/100 (budget)\n",
            "Adding requests: 100%|██████████████████████████| 1/1 [00:00<00:00, 1762.31it/s]\n",
            "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, o^C\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/eval/server/mmlu/src/telemetry/monitor.py\", line 288, in monitor_evaluation\n",
            "[rank0]:     yield monitor\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/eval/server/mmlu/src/evaluators/budget_evaluator.py\", line 174, in evaluate_subject\n",
            "[rank0]:     prediction = self.model.predict(\n",
            "[rank0]:                  ^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/eval/server/mmlu/src/models/vllm_model.py\", line 155, in predict\n",
            "[rank0]:     completions = self.model.generate([prompt], sampling_params)\n",
            "[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py\", line 1557, in inner\n",
            "[rank0]:     return fn(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py\", line 497, in generate\n",
            "[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)\n",
            "[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py\", line 1715, in _run_engine\n",
            "[rank0]:     step_outputs = self.llm_engine.step()\n",
            "[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py\", line 1221, in step\n",
            "[rank0]:     outputs = self.model_executor.execute_model(\n",
            "[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 147, in execute_model\n",
            "[rank0]:     output = self.collective_rpc(\"execute_model\",\n",
            "[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n",
            "[rank0]:     answer = run_method(self.driver_worker, method, args, kwargs)\n",
            "[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/vllm/utils/__init__.py\", line 3007, in run_method\n",
            "[rank0]:     return func(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 417, in execute_model\n",
            "[rank0]:     output = self.model_runner.execute_model(\n",
            "[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "[rank0]:     return func(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/vllm/worker/model_runner.py\", line 1763, in execute_model\n",
            "[rank0]:     output: SamplerOutput = self.sampler(\n",
            "[rank0]:                             ^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/sampler.py\", line 292, in forward\n",
            "[rank0]:     maybe_deferred_sample_results, maybe_sampled_tokens_tensor = _sample(\n",
            "[rank0]:                                                                  ^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/sampler.py\", line 752, in _sample\n",
            "[rank0]:     return _sample_with_torch(\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/sampler.py\", line 721, in _sample_with_torch\n",
            "[rank0]:     return get_pythonized_sample_results(\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/sampler.py\", line 598, in get_pythonized_sample_results\n",
            "[rank0]:     sample_results = _random_sample(seq_groups,\n",
            "[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/sampler.py\", line 490, in _random_sample\n",
            "[rank0]:     random_samples = random_samples.cpu()\n",
            "[rank0]:                      ^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]: KeyboardInterrupt\n",
            "\n",
            "[rank0]: During handling of the above exception, another exception occurred:\n",
            "\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/eval/server/mmlu/scripts/budget.py\", line 171, in <module>\n",
            "[rank0]:     success = main()\n",
            "[rank0]:               ^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/eval/server/mmlu/scripts/budget.py\", line 89, in main\n",
            "[rank0]:     result = evaluator.evaluate_subject(\n",
            "[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/eval/server/mmlu/src/evaluators/budget_evaluator.py\", line 155, in evaluate_subject\n",
            "[rank0]:     with monitor_evaluation(\n",
            "[rank0]:   File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
            "[rank0]:     self.gen.throw(value)\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/eval/server/mmlu/src/telemetry/monitor.py\", line 290, in monitor_evaluation\n",
            "[rank0]:     monitor.stop_monitoring()\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/eval/server/mmlu/src/telemetry/monitor.py\", line 116, in stop_monitoring\n",
            "[rank0]:     gpu_stats = self.nvml_handler.stop()\n",
            "[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/home/modfi/models/edgereasoning/eval/server/mmlu/src/telemetry/nvml_telemetry.py\", line 413, in stop\n",
            "[rank0]:     self._thread.join()\n",
            "[rank0]:   File \"/usr/lib/python3.12/threading.py\", line 1147, in join\n",
            "[rank0]:     self._wait_for_tstate_lock()\n",
            "[rank0]:   File \"/usr/lib/python3.12/threading.py\", line 1167, in _wait_for_tstate_lock\n",
            "[rank0]:     if lock.acquire(block, timeout):\n",
            "[rank0]:        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]: KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# Server evaluation options\n",
        "if platform == \"server\":\n",
        "    print(\"  SERVER EVALUATION OPTIONS\")\n",
        "    print(\"\\n Run evaluations using make commands:\")\n",
        "    \n",
        "    #!make help | grep -A10 \"Server Evaluations\"\n",
        "    \n",
        "    #!make server-mmlu\n",
        "    \n",
        "    #print(\"# !make planner\")\n",
        "    \n",
        "    !cd eval/server/mmlu && ./run.sh budget --model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
        "    \n",
        "else:\n",
        "    print(\"  Server evaluations not available on this platform\")\n",
        "    print(\"   Detected platform:\", platform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "server\n"
          ]
        }
      ],
      "source": [
        "platform = !make platform\n",
        "platform = platform[0].split(\":\")[-1].strip()\n",
        "print(platform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tegra Evaluations\n",
        "\n",
        "For Tegra platforms, we can run synthetic benchmarks and MMLU evaluations:\n",
        "\n",
        "### Available evaluations:\n",
        "- **base**: Base MMLU evaluation\n",
        "- **budget**: Budget evaluation\n",
        "- **scaling**: Test-time scaling evaluation  \n",
        "- **prefill**: Prefill synthetic benchmarks\n",
        "- **decode**: Decode synthetic benchmarks\n",
        "- **synthetic**: All synthetic benchmarks\n",
        "- **all**: All evaluations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tegra evaluation options  \n",
        "if platform == \"tegra\":\n",
        "    print(\"TEGRA EVALUATION OPTIONS\")\n",
        "    \n",
        "    print(\"\\nAvailable Tegra targets:\")\n",
        "    #!make help | grep -A15 \"Tegra/Jetson Evaluation\"\n",
        "    \n",
        "    #!make prefill\n",
        "    \n",
        "    #!make tegra-base\n",
        "    \n",
        "    #!cd eval/tegra && ./open.sh 1\n",
        "    \n",
        "    \n",
        "    print(f\"\\nResults will be saved to: data/mmlu/tegra/\")\n",
        "    \n",
        "else:\n",
        "    print(\"WARNING: Tegra evaluations not available on this platform\")\n",
        "    print(\"   Detected platform:\", platform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Results Processing\n",
        "\n",
        "After running evaluations, process the results to generate figures and coefficients for the analytical models:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"POST-PROCESSING WORKFLOW\")\n",
        "\n",
        "#Step 1: Process benchmark results\")\n",
        "# !python postprocess.py --results --sub-config prefill\n",
        "# !python postprocess.py --results --sub-config decode\n",
        "\n",
        "#Step 2: Generate analytical models\"\n",
        "# !cd third_party/token2metrics\n",
        "# !python -m prefilltokens.main\n",
        "# !python -m prefillenergy.cli --all\n",
        "# !python -m decodetokens.main\n",
        "# !python -m decodeenergy.cli --all\n",
        "\n",
        "\n",
        "#\"Step 3: Expected outputs\"\n",
        "#\"- Figures 1-5 in outputs/\"\n",
        "#\"- Fit coefficients for config/analytic.yaml\"\n",
        "#\"- Model parameters in validation/\"\n",
        "\n",
        "# Check if results directory exists\n",
        "results_base = Path(\"data\")\n",
        "if results_base.exists():\n",
        "    print(f\"\\nFound results directory: {results_base}\")\n",
        "    subdirs = [d for d in results_base.iterdir() if d.is_dir()]\n",
        "    if subdirs:\n",
        "        print(\"   Available result sets:\")\n",
        "        for subdir in subdirs[:5]:  \n",
        "            print(f\"   └── {subdir.name}/\")\n",
        "        if len(subdirs) > 5:\n",
        "            print(f\"   └── ... and {len(subdirs)-5} more\")\n",
        "    else:\n",
        "        print(\"   (Empty - run evaluations first)\")\n",
        "else:\n",
        "    print(\"\\nWARNING: No results directory found\")\n",
        "    print(\"   Run evaluations first to generate data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Analytical Models\n",
        "\n",
        "Test the analytical latency and energy prediction models with different input/output token combinations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python energy_model.py --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANALYTICAL MODEL TESTING\n",
            "python energy_model.py -i 128 -o 128\n",
            "python energy_model.py -i 256 -o 64\n",
            "python energy_model.py -i 512 -o 256\n",
            "python energy_model.py -i 1024 -o 512\n"
          ]
        }
      ],
      "source": [
        "print(\"ANALYTICAL MODEL TESTING\")\n",
        "test_cases = [\n",
        "    (128, 128),\n",
        "    (256, 64), \n",
        "    (512, 256),\n",
        "    (1024, 512)\n",
        "]\n",
        "\n",
        "# python latency_model.py -i {input_tokens} -o {output_tokens}\n",
        "\n",
        "for input_tokens, output_tokens in test_cases:\n",
        "    import subprocess\n",
        "    subprocess.run([\n",
        "        \"python\", \"energy_model.py\",\n",
        "        \"-i\", str(input_tokens),\n",
        "        \"-o\", str(output_tokens)\n",
        "    ])\n",
        "\n",
        "# You can modify these values and run:\n",
        "\n",
        "input_tokens = 384   \n",
        "output_tokens = 256 \n",
        "\n",
        "#Test with {input_tokens} input tokens, {output_tokens} output tokens:\n",
        "#python latency_model.py -i {input_tokens} -o {output_tokens}\n",
        "#python energy_model.py -i {input_tokens} -o {output_tokens}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Validation & Summary\n",
        "\n",
        "Validate the analytical models and review the complete workflow:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/modfi/models/edgereasoning\n",
            "Processing prefill results from Tegra...\n",
            "Input dir: ../../../data/synthetic/gpu/prefill\n",
            "Output dir: ../../../data/synthetic/gpu/prefill/processed\n",
            "🔋 Energy Analysis Pipeline\n",
            "==================================================\n",
            "Collecting energy files...\n",
            "No energy files found in the specified directory\n",
            "❌ No energy data found to analyze\n",
            "🔄 Energy-Performance Correlation Analysis\n",
            "============================================================\n",
            "❌ Error: Performance file not found: ../../../data/synthetic/gpu/prefill/processed/all_results_by_model_*.xlsx\n",
            "🔍 Power Insights Analysis\n",
            "========================================\n",
            "🔍 Auto-detected correlation file: /home/modfi/models/edgereasoning/third_party/token2metrics/prefillenergy/energy/../../decodenergy/output/energy_performance_correlation.xlsx\n",
            "Correlation file: /home/modfi/models/edgereasoning/third_party/token2metrics/prefillenergy/energy/../../decodenergy/output/energy_performance_correlation.xlsx\n",
            "Auto-detected correlation file: /home/modfi/models/edgereasoning/third_party/token2metrics/prefillenergy/energy/../../decodenergy/output/energy_performance_correlation.xlsx\n",
            "Loading correlation data from: /home/modfi/models/edgereasoning/third_party/token2metrics/prefillenergy/energy/../../decodenergy/output/energy_performance_correlation.xlsx\n",
            "  DeepSeek-R1-Distill-Qwen-1.5B: 168 questions loaded\n",
            "  DeepSeek-R1-Distill-Qwen-14B: 168 questions loaded\n",
            "Using all available data points (no token range filtering)\n",
            "Total questions found: 336\n",
            "Input token ranges: [np.int64(130), np.int64(258), np.int64(386), np.int64(514), np.int64(642), np.int64(770), np.int64(898), np.int64(1026)]\n",
            "Generating power consumption analysis...\n",
            "Generated analysis for 16 data points\n",
            "\n",
            "❌ Analysis failed: 'png'\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/modfi/models/edgereasoning/third_party/token2metrics/prefillenergy/energy/cli.py\", line 308, in main\n",
            "    run_insights_analysis(args.correlation_file, args.verbose)\n",
            "  File \"/home/modfi/models/edgereasoning/third_party/token2metrics/prefillenergy/energy/cli.py\", line 123, in run_insights_analysis\n",
            "    results = analyzer.run_complete_analysis(correlation_file)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/modfi/models/edgereasoning/third_party/token2metrics/prefillenergy/energy/insights.py\", line 53, in run_complete_analysis\n",
            "    power_chart = self.power_charts.create(analysis_df)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/modfi/models/edgereasoning/third_party/token2metrics/prefillenergy/energy/visualization/charts.py\", line 84, in create\n",
            "    print(f\"Power scaling chart saved: {paths['png']}\")\n",
            "                                        ~~~~~^^^^^^^\n",
            "KeyError: 'png'\n",
            "🔧 Energy and Power Fitting Analysis\n",
            "==================================================\n",
            "🔍 Auto-detected input file: /home/modfi/models/edgereasoning/third_party/token2metrics/prefillenergy/energy/../../decodenergy/output/energy_performance_correlation.xlsx\n",
            "❌ Error during fitting analysis: No module named 'pwlf'\n"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "!cd third_party/token2metrics/prefillenergy/ && ./run.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#call the run.sh script\n",
        "\n",
        "!cd third_party/token2metrics/prefillenergy/\n",
        "!./run.sh\n",
        "!cd third_party/token2metrics/decodeenergy/\n",
        "!./run.sh\n",
        "\n",
        "\n",
        "#Or run individual scripts\n",
        "\n",
        "# !cd third_party/token2metrics/prefillenergy/ && python generate_lookup_table.py\n",
        "# !cd third_party/token2metrics/decodeenergy/ && python generate_lookup_table.py\n",
        "# !cd third_party/token2metrics/decodeenergy/ && python empirical.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(f\"\\nCurrent Platform: {platform.upper()}\")\n",
        "print(f\"Repository Structure:\")\n",
        "print(\"   eval/server/     - Server MMLU & Planner evaluations\")\n",
        "print(\"   eval/tegra/      - Tegra containerized evaluations\")\n",
        "print(\"   data/            - All results organized by platform\")\n",
        "print(\"   third_party/     - Post-processing and model fitting\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
